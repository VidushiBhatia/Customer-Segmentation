{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Customer_Segmentation_Case_Study_Instacart.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VidushiBhatia/Customer-Segmentation/blob/main/Customer_Segmentation_Case_Study_Instacart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f256468c"
      },
      "source": [
        "# End-to-end segmentation implementation on [Instacart](https://tech.instacart.com/3-million-instacart-orders-open-sourced-d40d29ead6f2)"
      ],
      "id": "f256468c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13327d31"
      },
      "source": [
        "## 1. Install Relevant Packages"
      ],
      "id": "13327d31"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "067b35d2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale, StandardScaler\n",
        "from matplotlib.pyplot import plot\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install kneed\n",
        "from kneed import KneeLocator\n",
        "from sklearn.metrics import silhouette_score"
      ],
      "id": "067b35d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f7b2282"
      },
      "source": [
        "## 2. Read Files and Understand Data\n",
        "\n",
        "   * Read all the files\n",
        "   * Review Tables to get a sense of the data"
      ],
      "id": "5f7b2282"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV-SGcrYHQfI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d4c1db-888c-4b48-bcd5-712c418b36d4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "CV-SGcrYHQfI",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPhknE0VHpkY"
      },
      "source": [
        "############################################ Helper Function 1 ############################################################\n",
        "############################################### READ FILES ################################################################\n",
        "\n",
        "def read_data(path):\n",
        "  all_dfs = {}\n",
        "  df = pd.DataFrame()\n",
        "  files = os.listdir(path) \n",
        "  missing_value_formats = [\"n.a.\",\"?\",\"NA\",\"n/a\", \"na\", \"--\",\"-\"]\n",
        "  \n",
        "  for file in files:\n",
        "    if file.endswith('.csv'):\n",
        "      all_dfs[file[:file.find(\".\")]] = pd.read_csv(os.path.join(path,file), na_values = missing_value_formats)\n",
        "  return all_dfs\n",
        "\n",
        "############################################ Helper Function 2 ############################################################\n",
        "############################################ SUMMARIZE FILES ##############################################################\n",
        "\n",
        "def summarize_data(df):\n",
        "  print(\"\\nOverview\")\n",
        "  display(df.head())\n",
        "  print(\"\\nSummary\")\n",
        "  display(df.describe(include='all'))\n",
        "  print(\"\\nNull Values\")\n",
        "  display(df.isnull().sum()/len(df))"
      ],
      "id": "RPhknE0VHpkY",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d80ea80b"
      },
      "source": [
        "# Understand the available data\n",
        "\n",
        "path = '/content/drive/MyDrive/instacart-market-basket-analysis/'\n",
        "all_dfs = read_data(path)\n",
        "\n",
        "print(\"\\n############################## AISLES ##############################\")\n",
        "summarize_data(all_dfs['aisles'])\n",
        "print(\"\\n############################## DEPARTMENTS ##############################\")\n",
        "summarize_data(all_dfs['departments'])\n",
        "print(\"\\n############################## ORDER-PRODUCTS PRIOR ##############################\")\n",
        "summarize_data(all_dfs['order_products__prior'])\n",
        "print(\"\\n############################## ORDER-PRODUCTS TRAIN ##############################\")\n",
        "summarize_data(all_dfs['order_products__train'])\n",
        "print(\"\\n############################## ORDERS ##############################\")\n",
        "summarize_data(all_dfs['orders'])\n",
        "print(\"\\n############################## PRODUCTS ##############################\")\n",
        "summarize_data(all_dfs['products'])\n",
        "print(\"\\n############################## SAMPLE SUBMISSION ##############################\")\n",
        "summarize_data(all_dfs['sample_submission'])\n"
      ],
      "id": "d80ea80b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKEJmcWcoA6w"
      },
      "source": [
        "# MISSING VALUE TREATMENT\n",
        "\n",
        "# Checking reason for Missing values and applying apt missing value treatment\n",
        "\n",
        "all_dfs['orders'].loc[all_dfs['orders']['order_number']==1, 'days_since_prior_order'] = 0\n",
        "summarize_data(all_dfs['orders'])"
      ],
      "id": "RKEJmcWcoA6w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ffe201"
      },
      "source": [
        "<br>\n",
        "\n",
        "**Overview of Tables**\n",
        "\n",
        "| Table Name | Column Names | Description | Hypothesis for Data Exploration  |\n",
        "| --- | --- | --- | --- | \n",
        "| aisles | aisle_id, aisle | 134 unique IDs, names for different aisles at Instacart | aisle which generates highest revenue, most frequently used aisle, distribution of<br> aisle usage based on demographics and other customer info, is there a trend of<br> aisle popularity with time of day, day of week, or any specific month of year? |\n",
        "| departments | department_id, department | 21 unique IDs, names for different departments including 'missing'<br> - looks like the rolled-up metric for aisles e.g. one department might <br> have multiple aisles | revenue by dept, frequency of use by dept, distribution of dept usage based<br> on demographics and other available customer info, dept popularity by time <br>of day, day of week, or any specific month of year  |\n",
        "| order-products-prior | order_id, product_id, add_to_cart_order, reordered | 32,434,489 rows at order-product level with 3,214,874 unique orders <br>for 49,677 unique products. 'add_to_cart_order' shows the order in which<br> they were added to the cart and 'reordered' | most ordered products, most frequent re-ordered products, products which <br>are only ordered once and not reordered, Number of products in one order,<br> are the products that are ordered together from the same aisle/dept? |\n",
        " | order-products-train | order_id, product_id, add_to_cart_order, reordered | Similar to df_prior_orders but only has latest order information. 1,384,617 rows<br> with 131,209 unique order IDs and 39,123  | most ordered products, most frequent re-ordered products, products <br>which are only ordered once and not reordered, Number of products in <br>one order, are the products that are ordered together from the same aisle/dept? |\n",
        "| orders | order_id, user_id, eval_set,<br> order_number, order_dow,<br> order_hour_of_day, days_since_prior_order | 3,421,083 orders showing information on order ID, user ID, which evaluation<br> dataset the order is in (prior, train, test), day of week, hour of day,<br> days since prior order | Most popular day & time for placing an order, trend of day & time by products |\n",
        "| products | product_id, product_name, aisle_id, department_id | 49,688 rows mapping products to aisles and departments |\tcovered in above metrics |\n",
        "\n",
        "<br>\n",
        "\n",
        "**Missing Values**\n",
        "\n",
        "* 6% values are missing for 'days since prior order' in 'orders' df --> Given these are only for Order Number 1, we can replace missing values with '0' days\n",
        "* No other missing values found in the data\n",
        "* FYI - We can use Imputers from sklearn in couldn't replace our missing data with 0\n",
        "\n"
      ],
      "id": "44ffe201"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul-OuWvlQuCm"
      },
      "source": [
        "## 3. Create a Master Dataset"
      ],
      "id": "ul-OuWvlQuCm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV-lCpSCPTMj"
      },
      "source": [
        "# Prior and Train order datasets have the same columns orders from different time frame - These need to be concatenated\n",
        "\n",
        "master_df = pd.concat([all_dfs['order_products__prior'], all_dfs['order_products__train']]).sort_values(by=['order_id'])\n",
        "\n",
        "# Merge the rest of the datasets\n",
        "master_df = pd.merge(left = master_df, right = all_dfs['products'],\n",
        "                             left_on='product_id', right_on='product_id').sort_values(by=['order_id']).reset_index(drop=True)\n",
        "master_df = pd.merge(left = master_df, right = all_dfs['aisles'],\n",
        "                             left_on='aisle_id', right_on='aisle_id').sort_values(by=['order_id']).reset_index(drop=True)\n",
        "master_df = pd.merge(left = master_df, right = all_dfs['departments'],\n",
        "                             left_on='department_id', right_on='department_id').sort_values(by=['order_id']).reset_index(drop=True)\n",
        "master_df = pd.merge(left = master_df, right = all_dfs['orders'],\n",
        "                             left_on='order_id', right_on='order_id').sort_values(by=['order_id']).reset_index(drop=True)\n",
        "\n",
        "col_order = ['user_id','order_id','product_id','aisle_id','department_id','add_to_cart_order',\n",
        " 'reordered','product_name','aisle','department','eval_set','order_number','order_dow','order_hour_of_day',\n",
        " 'days_since_prior_order']\n",
        "\n",
        "master_df = master_df[col_order]\n",
        "\n",
        "del all_dfs\n",
        "\n",
        "summarize_data(master_df)"
      ],
      "id": "uV-lCpSCPTMj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdTbcSyUofIq"
      },
      "source": [
        "## 4. EDA"
      ],
      "id": "NdTbcSyUofIq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B8WO698x-g-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5748a659-8cfe-40cc-c664-ae74fa907a36"
      },
      "source": [
        "# Identify Primary Key\n",
        "len(master_df.groupby(['user_id','order_id','product_id'], as_index=False).count())"
      ],
      "id": "3B8WO698x-g-",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33819106"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7RybYG4grRc"
      },
      "source": [
        "# Identify unique values in columns\n",
        "print (\"\\nNumber of Rows : \", len(master_df))\n",
        "print (\"Unique user_id: \", master_df['user_id'].nunique(),\", % :\", f\"{master_df['user_id'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique order_id: \", master_df['order_id'].nunique(),\", % :\", f\"{master_df['order_id'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique product_id: \", master_df['product_id'].nunique(),\", % :\", f\"{master_df['product_id'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique aisle_id: \", master_df['aisle_id'].nunique(),\", % :\", f\"{master_df['aisle_id'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique department_id: \", master_df['department_id'].nunique(),\", % :\", f\"{master_df['department_id'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique add_to_cart_order: \", master_df['add_to_cart_order'].nunique(),\", % :\", f\"{master_df['add_to_cart_order'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique reordered: \", master_df['reordered'].nunique(),\", % :\", f\"{master_df['reordered'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique product_name: \", master_df['product_name'].nunique(),\", % :\", f\"{master_df['product_name'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique aisle: \", master_df['aisle'].nunique(),\", % :\", f\"{master_df['aisle'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique department: \", master_df['department'].nunique(),\", % :\", f\"{master_df['department'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique eval_set: \", master_df['eval_set'].nunique(),\", % :\", f\"{master_df['eval_set'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique order_number: \", master_df['order_number'].nunique(),\", % :\", f\"{master_df['order_number'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique order_dow: \", master_df['order_dow'].nunique(),\", % :\", f\"{master_df['order_dow'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique order_hour_of_day: \", master_df['order_hour_of_day'].nunique(),\", % :\", f\"{master_df['order_hour_of_day'].nunique() / len(master_df): .2%}\")\n",
        "print (\"Unique days_since_prior_order: \", master_df['days_since_prior_order'].nunique(),\", % :\", f\"{master_df['days_since_prior_order'].nunique() / len(master_df): .2%}\")"
      ],
      "id": "b7RybYG4grRc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIWM1adOBCQa"
      },
      "source": [
        "# look at the heirarchy of products, aisles and departments\n",
        "\n",
        "master_df.groupby(['department','aisle','product_name'], as_index = False).size()"
      ],
      "id": "OIWM1adOBCQa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "415cc3be"
      },
      "source": [
        "############################################ Helper Function 3 ############################################################\n",
        "############################################ VISUALIZE DATA ##############################################################\n",
        "\n",
        "def CreateCharts (ax, data, x, y, chart_type, legend = False, size = 5, hue = None, palette=None):\n",
        "  if chart_type == \"scatter\":\n",
        "      plot = sns.scatterplot(data=data, x=x, y=y, size=size, legend=legend, \n",
        "              hue=hue, sizes=(20, 200), palette = palette, ax=ax)\n",
        "  elif chart_type == \"bar\":\n",
        "      plot = sns.barplot(x=x, y=y, data=data, \n",
        "              hue=hue, palette = palette, ax=ax, ci=\"sd\")\n",
        "  elif chart_type == \"density\":\n",
        "      plot = sns.kdeplot(x=x, data=data, \n",
        "              shade=True, alpha=0.5, ax=ax)  \n",
        "  elif chart_type == \"swarm\":\n",
        "      plot = sns.swarmplot(x=x, y=y, hue=hue, data=data,\n",
        "                    palette=palette, ax=ax)\n",
        "  elif chart_type =='hist':\n",
        "      plot = sns.histplot(data=data, x=x, kde=True, ax = ax, palette = palette )\n",
        "  return plot"
      ],
      "id": "415cc3be",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe468ac5"
      },
      "source": [
        "\n",
        "sns.set(style=\"darkgrid\")\n",
        "fig, ax = plt.subplots(4,3, figsize=(25,15))\n",
        "\n",
        "# Plot 1\n",
        "data = master_df[['user_id','order_id']].drop_duplicates().groupby('user_id').size().reset_index(name='Number of Orders').sort_values(by='Number of Orders', ascending=False)\n",
        "ax[0,0] = CreateCharts(ax[0,0], data, x = \"Number of Orders\", y=None, chart_type = \"hist\", palette=\"bright\")\n",
        "ax[0,0].set_title('How many orders are made by a single user?', fontsize=10)\n",
        "\n",
        "\n",
        "# Plot 2\n",
        "data = master_df.groupby('order_id').size().reset_index(name='Number of Products').sort_values(by='Number of Products', ascending=False)\n",
        "ax[0,1] = CreateCharts(ax[0,1], data, x = \"Number of Products\", y=None, chart_type = \"hist\", palette=\"dark\")\n",
        "ax[0,1].set_title('How many products are in a single order?', fontsize=10)\n",
        "\n",
        "\n",
        "# Plot 3\n",
        "import matplotlib.patches as mpatches\n",
        "data1 = master_df.groupby('days_since_prior_order', as_index=False).size()\n",
        "bar1 = sns.barplot(x=\"days_since_prior_order\",  y=\"size\", data=data1, color='darkblue', ax=ax[0,2])\n",
        "data2 = master_df[master_df['reordered']==1].groupby('days_since_prior_order', as_index=False).size()\n",
        "bar2 = sns.barplot(x=\"days_since_prior_order\",  y=\"size\", data=data2,  color='lightblue', ax=ax[0,2])\n",
        "top_bar = mpatches.Patch(color='darkblue', label='total orders')\n",
        "bottom_bar = mpatches.Patch(color='lightblue', label='reordered')\n",
        "ax[0,2].legend(handles=[top_bar, bottom_bar])\n",
        "ax[0,2].set_title('How many days have passed since the last order?', fontsize=10)\n",
        "ax[0,2].set_ylabel('count')\n",
        "\n",
        "\n",
        "# Plot 4\n",
        "\n",
        "data1 = master_df.groupby('order_dow', as_index=False).size()\n",
        "bar1 = sns.barplot(x=\"order_dow\",  y=\"size\", data=data1, color='maroon', ax=ax[1,0])\n",
        "data2 = master_df[master_df['reordered']==1].groupby('order_dow', as_index=False).size()\n",
        "bar2 = sns.barplot(x=\"order_dow\",  y=\"size\", data=data2,  color='yellow', ax=ax[1,0])\n",
        "top_bar = mpatches.Patch(color='maroon', label='total orders')\n",
        "bottom_bar = mpatches.Patch(color='yellow', label='reordered')\n",
        "ax[1,0].legend(handles=[top_bar, bottom_bar])\n",
        "ax[1,0].set_title('Which days have the most orders/reorders?', fontsize=10)\n",
        "ax[1,0].set_ylabel('count')\n",
        "\n",
        "# Plot 5\n",
        "data1 = master_df.groupby('order_hour_of_day', as_index=False).size()\n",
        "bar1 = sns.barplot(x=\"order_hour_of_day\",  y=\"size\", data=data1, color='purple', ax=ax[1,1])\n",
        "data2 = master_df[master_df['reordered']==1].groupby('order_hour_of_day', as_index=False).size()\n",
        "bar2 = sns.barplot(x=\"order_hour_of_day\",  y=\"size\", data=data2,  color='lightgreen', ax=ax[1,1])\n",
        "top_bar = mpatches.Patch(color='purple', label='total orders')\n",
        "bottom_bar = mpatches.Patch(color='lightgreen', label='reordered')\n",
        "ax[1,1].legend(handles=[top_bar, bottom_bar])\n",
        "ax[1,1].set_title('Which hour of day has the most orders/reorders?', fontsize=10)\n",
        "ax[1,1].set_ylabel('count')\n",
        "\n",
        "# Plot 6\n",
        "data = master_df.groupby('order_id', as_index = False)['add_to_cart_order'].max()\n",
        "ax[1,2] = CreateCharts(ax[1,2], data, x = \"add_to_cart_order\", y=None, chart_type = \"hist\", palette=\"bright\")\n",
        "ax[1,2].set_title('What is the cart size across orders?', fontsize=10)\n",
        "\n",
        "\n",
        "# Plot 7\n",
        "data = master_df.groupby('product_name', as_index = False)['product_id'].count().sort_values(by='product_id', ascending=False)\n",
        "ax[2,0] = CreateCharts(ax[2,0], data.head(25), \"product_name\", \"product_id\", \"bar\", palette = \"Set2\")\n",
        "ax[2,0].set_xticklabels(data.head(25)['product_name'], rotation = 90)\n",
        "ax[2,0].set_title('What are the most ordered products?', fontsize=10)\n",
        "ax[2,0].set_xlabel('')\n",
        "ax[2,0].set_ylabel('count')\n",
        "\n",
        "# Plot 8\n",
        "data = master_df.groupby('aisle', as_index = False)['aisle_id'].count().sort_values(by='aisle_id', ascending=False)\n",
        "ax[2,1] = CreateCharts(ax[2,1], data.head(25), \"aisle\", \"aisle_id\", \"bar\", palette = \"Set2\")\n",
        "ax[2,1].set_xticklabels(data.head(25)['aisle'], rotation = 90)\n",
        "ax[2,1].set_title('Which aisle do users order the most from?', fontsize=10)\n",
        "ax[2,1].set_xlabel('')\n",
        "ax[2,1].set_ylabel('count')\n",
        "\n",
        "# Plot 9\n",
        "data = master_df.groupby('department', as_index = False)['department_id'].count().sort_values(by='department_id', ascending=False)\n",
        "ax[2,2] = CreateCharts(ax[2,2], data.head(25), \"department\", \"department_id\", \"bar\", palette = \"Set2\")\n",
        "ax[2,2].set_xticklabels(data.head(25)['department'], rotation = 90)\n",
        "ax[2,2].set_title('Which department do users order the most from?', fontsize=10)\n",
        "ax[2,2].set_xlabel('')\n",
        "ax[2,2].set_ylabel('count')\n",
        "\n",
        "\n",
        "# Plot 10\n",
        "data = master_df.groupby('product_name', as_index = False)['reordered'].sum().sort_values(by = 'reordered', ascending=False)\n",
        "ax[3,0] = CreateCharts(ax[3,0], data.head(25),'product_name' , \"reordered\", \"bar\", palette = \"hls\")\n",
        "ax[3,0].set_xticklabels(data.head(25)['product_name'], rotation = 90)\n",
        "ax[3,0].set_title('What are the most reordered products?', fontsize=10)\n",
        "ax[3,0].set_xlabel('')\n",
        "\n",
        "# Plot 11\n",
        "data = master_df.groupby('aisle', as_index = False)['reordered'].sum().sort_values(by = 'reordered', ascending=False)\n",
        "ax[3,1] = CreateCharts(ax[3,1], data.head(25),'aisle' , \"reordered\", \"bar\", palette = \"hls\")\n",
        "ax[3,1].set_xticklabels(data.head(25)['aisle'], rotation = 90)\n",
        "ax[3,1].set_title('Which aisle has the most reorders?', fontsize=10)\n",
        "ax[3,1].set_xlabel('')\n",
        "\n",
        "# Plot 12\n",
        "data = master_df.groupby('department', as_index = False)['reordered'].sum().sort_values(by = 'reordered', ascending=False)\n",
        "ax[3,2] = CreateCharts(ax[3,2], data.head(25),'department' , \"reordered\", \"bar\", palette = \"hls\")\n",
        "ax[3,2].set_xticklabels(data.head(25)['department'], rotation = 90)\n",
        "ax[3,2].set_title('Which department has the most reorders?', fontsize=10)\n",
        "ax[3,2].set_xlabel('')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(left=0.125,\n",
        "                    bottom=0.1, \n",
        "                    right=0.9, \n",
        "                    top=0.9, \n",
        "                    wspace=0.2, \n",
        "                    hspace=1.5)\n",
        "\n",
        "plt.show()"
      ],
      "id": "fe468ac5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei_LzCFrt40V"
      },
      "source": [
        "# Correlation\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.title('Pearson Correlation of Features', size = 15)\n",
        "colormap = sns.diverging_palette(10, 220, as_cmap = True)\n",
        "sns.heatmap(master_df.corr(),\n",
        "            cmap = colormap,\n",
        "            square = True,\n",
        "            annot = True,\n",
        "            linewidths=0.1,vmax=1.0, linecolor='white',\n",
        "            annot_kws={'fontsize':12 })\n",
        "plt.show()"
      ],
      "id": "ei_LzCFrt40V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df259f88"
      },
      "source": [
        "## 5. Feature Engineering"
      ],
      "id": "df259f88"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_tWAGt2MmWn"
      },
      "source": [
        "############################################ Helper Function 4 ############################################################\n",
        "################################################# RUN PCA #################################################################\n",
        "\n",
        "def run_PCA(data, features, name):\n",
        "  pca = PCA(n_components=features)\n",
        "  scale_data = pd.DataFrame(scale(data), columns = data.columns, index = data.index)\n",
        "  pca_output = pca.fit_transform(data)\n",
        "  df_pca = pd.DataFrame(data = pca_output, columns = [name+str(i) for i in range(features)], index = data.index)\n",
        "  plot(pca.explained_variance_ratio_.cumsum(), linewidth=2)\n",
        "  print(\"\\nImportance of Components\")\n",
        "  print(pd.DataFrame(data = pca.components_, columns = data.columns, index = ['prod_'+str(i) for i in range(features)]))\n",
        "  return df_pca"
      ],
      "id": "e_tWAGt2MmWn",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOEB8KD9JFwy"
      },
      "source": [
        "# PCA to find product preference\n",
        "prod_pref = pd.pivot_table(master_df.groupby(['user_id','aisle'], as_index=False).size(), values='size', index='user_id',\n",
        "                    columns=['aisle'], aggfunc=np.sum, fill_value=0)\n",
        "prod_pref = run_PCA(prod_pref, 10, \"prod_pref\")\n",
        "\n",
        "# PCA to find day preference\n",
        "# day_pref = pd.pivot_table(master_df, values = 'order_dow', index = 'user_id',\n",
        "#                columns = ['department'], aggfunc=np.median, fill_value=-1)\n",
        "\n",
        "# day_pref = run_PCA(day_pref, 10, \"day_pref\")"
      ],
      "id": "cOEB8KD9JFwy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpg3ILNx6w8G"
      },
      "source": [
        "dept_pref = pd.pivot_table(master_df.groupby(['user_id','department'], as_index=False).size(), values='size', index='user_id',\n",
        "                    columns=['department'], aggfunc=np.sum, fill_value=0)\n",
        "dept_pref['total'] = dept_pref.sum(axis = 1)\n",
        "\n",
        "for dept in dept_pref.columns:\n",
        "  dept_pref[dept+'_perc'] = dept_pref[dept] / dept_pref['total']\n",
        "\n",
        "dept_pref"
      ],
      "id": "mpg3ILNx6w8G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rJzJlUq9vfN"
      },
      "source": [
        "# Create features to demonstrate trend of products prefered by users\n",
        "model_input = prod_pref.merge(dept_pref, how='left', on='user_id')\n",
        "# Create features to demonstrate trends of orders made by users\n",
        "model_input = model_input.merge(master_df.groupby('user_id', as_index = False).agg(\n",
        "              {'order_id':'count',\n",
        "               'product_id':'count',\n",
        "               'days_since_prior_order':'mean',\n",
        "               'add_to_cart_order':'median',\n",
        "               }).rename(\n",
        "              columns={'order_id':'total_orders',\n",
        "                       'product_id':'total_products',\n",
        "                       'days_since_prior_order':'mean_days_since_prior_order',\n",
        "                       'add_to_cart_order':'median_cart_size'}),\n",
        "               how='left', on='user_id') \n",
        "model_input = model_input.set_index('user_id') \n",
        "model_input"
      ],
      "id": "1rJzJlUq9vfN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McZEZ4KCnW2g"
      },
      "source": [
        "## Make data suitable for chosen model\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# check for outliers \n",
        "for col in model_input.columns:\n",
        "  cap_val =  (np.mean(model_input[col]) + 3*np.std(model_input[col]))\n",
        "  model_input[col+\"_capped\"] = 0\n",
        "  model_input[col+\"_capped\"][model_input[col] >= 0 ] = np.minimum(cap_val,model_input[col][model_input[col] >= 0 ])\n",
        "  model_input[col+\"_capped\"][model_input[col] < 0 ] = np.maximum((-1)*cap_val,model_input[col][model_input[col] < 0 ])\n",
        "  print(\"\\nOutliers\", col)\n",
        "  print (\"cap: \", cap_val)\n",
        "  print (\"capped positive:\", sum (model_input[col][model_input[col] >= 0 ] > cap_val))\n",
        "  print (\"capped negative:\", sum (model_input[col][model_input[col] < 0 ] < (-1)*cap_val))\n",
        "\n",
        "\n",
        "# normalize data\n",
        "scaler = StandardScaler()\n",
        "model_input_scaled = pd.DataFrame(scaler.fit_transform(model_input),columns = model_input.columns, index = model_input.index)"
      ],
      "id": "McZEZ4KCnW2g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K-hdUiQXlSv"
      },
      "source": [
        "model_input_scaled.columns"
      ],
      "id": "_K-hdUiQXlSv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u_BBsME1UWZ"
      },
      "source": [
        "# final dataset\n",
        "\n",
        "X = model_input_scaled[['prod_pref0_capped',\n",
        "       'prod_pref1_capped', 'prod_pref2_capped', 'prod_pref3_capped',\n",
        "       'prod_pref4_capped', 'prod_pref5_capped', 'prod_pref6_capped', \n",
        "       'prod_pref7_capped', 'prod_pref8_capped', 'prod_pref9_capped',\n",
        "       'alcohol_perc', 'babies_perc', 'bakery_perc', 'beverages_perc',\n",
        "       'breakfast_perc', 'bulk_perc', 'canned goods_perc', 'dairy eggs_perc',\n",
        "       'deli_perc', 'dry goods pasta_perc', 'frozen_perc', 'household_perc',\n",
        "       'international_perc', 'meat seafood_perc', 'missing_perc', 'other_perc',\n",
        "       'pantry_perc', 'personal care_perc', 'pets_perc', 'produce_perc',\n",
        "       'snacks_perc', 'total_orders_capped',\n",
        "       'total_products_capped', 'mean_days_since_prior_order_capped',\n",
        "       'median_cart_size_capped']]\n",
        "\n",
        "# sns.pairplot(X)"
      ],
      "id": "_u_BBsME1UWZ",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPERco_b1VpK"
      },
      "source": [
        "## 6. Modeling - k-means Clustering"
      ],
      "id": "qPERco_b1VpK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqE2wLkR1aTn"
      },
      "source": [
        "# # find the right number of clusters - Elbow Method\n",
        "\n",
        "cluster_range = range(2,16)\n",
        "\n",
        "kmeans_kwargs = {\n",
        "      \"init\": \"k-means++\",\n",
        "      \"n_init\": 10,\n",
        "      \"max_iter\": 300,\n",
        "      \"random_state\": None,\n",
        "    }\n",
        "   \n",
        "# A list holds the SSE values for each k\n",
        "sse = []\n",
        "\n",
        "for k in cluster_range:\n",
        "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
        "    kmeans.fit(X)\n",
        "    sse.append(kmeans.inertia_)\n"
      ],
      "id": "EqE2wLkR1aTn",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnoTBImVn3iJ"
      },
      "source": [
        "kn = KneeLocator(\n",
        "    cluster_range, sse, curve=\"convex\", direction=\"decreasing\")\n",
        "plt.plot(cluster_range,sse)\n",
        "plt.title('The Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('SSE')\n",
        "plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')"
      ],
      "id": "ZnoTBImVn3iJ",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SptQlYL5Ikn5"
      },
      "source": [
        "# find the right number of clusters - Silhouette Score Method\n",
        "\n",
        "silhouette_coefficients = []\n",
        "cluster_range = range(3,10)\n",
        "\n",
        "for k in cluster_range:\n",
        "     kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
        "     kmeans.fit(X)\n",
        "     score = silhouette_score(X, kmeans.labels_)\n",
        "     silhouette_coefficients.append(score)\n",
        "\n",
        "plt.plot(cluster_range, silhouette_coefficients)\n",
        "plt.xticks(cluster_range)\n",
        "plt.xlabel(\"Number of Clusters\")\n",
        "plt.ylabel(\"Silhouette Coefficient\")\n",
        "plt.show()"
      ],
      "id": "SptQlYL5Ikn5",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2kRnK961kGL"
      },
      "source": [
        "# run model\n",
        "\n",
        "n_clusters = 4\n",
        "kmeans = KMeans(n_clusters=n_clusters, **kmeans_kwargs)\n",
        "kmeans.fit(X)\n",
        "print(\"\\nClusters\",n_clusters)\n",
        "print(\"\\nsse\",kmeans.inertia_)\n",
        "print(\"\\nIterations it took to converge: \", kmeans.n_iter_)\n",
        "# print(\"\\ncentroid locations: \",kmeans.cluster_centers_)\n",
        "model_input['segment_'+str(n_clusters)] = kmeans.labels_\n",
        "\n",
        "\n",
        "n_clusters = 5\n",
        "kmeans = KMeans(n_clusters=n_clusters, **kmeans_kwargs)\n",
        "kmeans.fit(X)\n",
        "print(\"\\nClusters\",n_clusters)\n",
        "print(\"\\nsse\",kmeans.inertia_)\n",
        "print(\"\\nIterations it took to converge: \", kmeans.n_iter_)\n",
        "# print(\"\\ncentroid locations: \",kmeans.cluster_centers_)\n",
        "model_input['segment_'+str(n_clusters)] = kmeans.labels_\n",
        "\n",
        "n_clusters = 6\n",
        "kmeans = KMeans(n_clusters=n_clusters, **kmeans_kwargs)\n",
        "kmeans.fit(X)\n",
        "print(\"\\nClusters\",n_clusters)\n",
        "print(\"\\nsse\",kmeans.inertia_)\n",
        "print(\"\\nIterations it took to converge: \", kmeans.n_iter_)\n",
        "# print(\"\\ncentroid locations: \",kmeans.cluster_centers_)\n",
        "model_input['segment_'+str(n_clusters)]= kmeans.labels_\n",
        "\n",
        "model_output = model_input"
      ],
      "id": "u2kRnK961kGL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypI5v-AsMZPq"
      },
      "source": [
        "## 7. Profiling"
      ],
      "id": "ypI5v-AsMZPq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEF18BGhuI6K"
      },
      "source": [
        "# clean our irrelavant datasets\n",
        "\n",
        "# df_list = [var for var in dir() if isinstance(eval(var), pd.core.frame.DataFrame)]\n",
        "# print(df_list)\n",
        "del model_input\n",
        "del data\n",
        "del data1\n",
        "del data2\n",
        "# del day_pref\n",
        "del model_input_scaled\n",
        "del prod_pref\n",
        "del X\n",
        "# del _9\n",
        "del _16\n",
        "# del _15\n",
        "del _10\n",
        "# del _22\n",
        "del _17\n",
        "del __\n",
        "del ___\n",
        "df_list = [var for var in dir() if isinstance(eval(var), pd.core.frame.DataFrame)]\n",
        "print(df_list)\n"
      ],
      "id": "JEF18BGhuI6K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnjwMyykMc71"
      },
      "source": [
        "# create master dataset with output\n",
        "master_df = master_df.merge(model_output[['total_orders_capped',\n",
        "       'total_products_capped', 'mean_days_since_prior_order_capped',\n",
        "       'median_cart_size_capped',\n",
        "        'alcohol_perc', 'babies_perc', 'bakery_perc', 'beverages_perc',\n",
        "       'breakfast_perc', 'bulk_perc', 'canned goods_perc', 'dairy eggs_perc',\n",
        "       'deli_perc', 'dry goods pasta_perc', 'frozen_perc', 'household_perc',\n",
        "       'international_perc', 'meat seafood_perc', 'missing_perc', 'other_perc',\n",
        "       'pantry_perc', 'personal care_perc', 'pets_perc', 'produce_perc',\n",
        "       'snacks_perc', 'segment_4', 'segment_5', 'segment_6']],\n",
        "        how='left', on='user_id')"
      ],
      "id": "YnjwMyykMc71",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyRvl-TeeDAh"
      },
      "source": [
        "master_df"
      ],
      "id": "LyRvl-TeeDAh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktHch9myUHIi"
      },
      "source": [
        "# Distribution with other segments (K+1, K-1)\n",
        "display(pd.crosstab(master_df['segment_4'], master_df['segment_5'], margins = False).apply(lambda r: round(r/r.sum(), 2), axis=1))\n",
        "pd.crosstab(master_df['segment_4'], master_df['segment_6'], margins = False).apply(lambda r: round(r/r.sum(), 2), axis=1)"
      ],
      "id": "ktHch9myUHIi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIshZTzqPxEa"
      },
      "source": [
        "def SegmentSummary(n):\n",
        "  segment = 'segment_'+str(n)\n",
        "  fig, ax = plt.subplots(1,n, figsize=(22,3))\n",
        "  \n",
        "  data = master_df.groupby([segment, 'aisle'], as_index = False).size().sort_values(by='size', ascending=False)\n",
        "  for i in range(n):\n",
        "    ax[i] = CreateCharts(ax[i], data[data[segment]==i].head(5), \"aisle\", \"size\", \"bar\", legend=False, size=None,hue=None, palette = \"dark\")\n",
        "    ax[i].set_xticklabels(data[data[segment]==i].head(5)['aisle'], rotation = 90)\n",
        "    ax[i].set_xlabel('')\n",
        "    ax[i].set_title('Segment '+str(i), fontsize=10)\n",
        "    ax[i].set_ylabel('')\n",
        "  plt.show()\n",
        "  \n",
        "  fig, ax = plt.subplots(1,n, figsize=(22,3))\n",
        "  data = master_df.groupby([segment, 'department'], as_index = False).size().sort_values(by='size', ascending=False)\n",
        "  for i in range(n):\n",
        "    ax[i] = CreateCharts(ax[i], data[data[segment]==i].head(5), \"department\", \"size\", \"bar\", legend=False, size=None,hue=None, palette = \"Set2\")\n",
        "    ax[i].set_xticklabels(data[data[segment]==i].head(5)['department'], rotation = 90)\n",
        "    ax[i].set_xlabel('')\n",
        "    ax[i].set_title('Segment '+str(i), fontsize=10)\n",
        "    ax[i].set_ylabel('')\n",
        "  plt.show()  \n",
        "\n",
        "  summary = master_df.groupby(segment, as_index = False).agg(\n",
        "              { segment: 'count', \n",
        "                'reordered':'sum',\n",
        "               'order_dow': 'median',\n",
        "               'order_hour_of_day':'median',\n",
        "               'days_since_prior_order': 'median',\n",
        "               'total_orders_capped': ['sum', 'mean'],\n",
        "               'total_products_capped': ['sum','mean'],\n",
        "               'mean_days_since_prior_order_capped':'mean',\n",
        "               'median_cart_size_capped':'median',\n",
        "               }).astype(float)\n",
        "\n",
        "  summary['n_size_perc'] = [round(i/sum(summary[segment]['count']),2) for i in summary[segment]['count']]\n",
        "  summary['reordered_perc'] = [round(i/sum(summary['reordered']['sum']),2) for i in summary['reordered']['sum']]\n",
        "  summary['total_orders_capped_perc'] = [round(i/sum(summary['total_orders_capped']['sum']),2) for i in summary['total_orders_capped']['sum']]\n",
        "  summary['total_products_capped_perc'] = [round(i/sum(summary['total_products_capped']['sum']),2) for i in summary['total_products_capped']['sum']]\n",
        "  display(summary)\n",
        "  return summary"
      ],
      "id": "ZIshZTzqPxEa",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5OcPecNtUTF"
      },
      "source": [
        "pd.options.display.float_format = '{:,}'.format\n",
        "summary_4 = SegmentSummary(4)\n",
        "summary_5 = SegmentSummary(5)\n",
        "summary_6 = SegmentSummary(6)\n",
        "\n",
        "# from google.colab import files\n",
        "# master_df.to_csv(\"master_df.csv\")\n",
        "# files.download('master_df.csv')"
      ],
      "id": "k5OcPecNtUTF",
      "execution_count": null,
      "outputs": []
    }
  ]
}